{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43796390",
   "metadata": {},
   "source": [
    "This project is a work in progress. I want to add a feature that can predict how a misspelled word would be pronounced using the linguistic rules of English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358caac",
   "metadata": {},
   "source": [
    "This first cell contains the necessary libraries for this Jupyter notebook, as well as configuration variables to fine-tune the functions in the sentence prediction/spellchecking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc069ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict, brown, reuters, stopwords\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nltk.download(\"cmudict\")\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger', force=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', force=True)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ---------- configuration ----------\n",
    "LAMBDA = [0.05, 0.15, 0.3, 0.5] # format: [unigram_weight, bigram_weight, trigram_weight, 4gram_weight]; sum(LAMBDA) ≈ 1.0; longer n-grams have more weight\n",
    "MAX_NGRAM = len(LAMBDA)\n",
    "SMOOTH = 1e-8 # generally a fallback value in case an n-gram has a count of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b339634",
   "metadata": {},
   "source": [
    "This second cell contains all of the functions that precompute variables that are used in the model -- such as the full corpus, n-gram counts, sum of the number of each n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d765e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_corpora(corpus_one, corpus_two, limit=None):\n",
    "    \"\"\"\n",
    "    Combine two corpora into a single list of lower-cased words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_one : NLTK corpus\n",
    "        the first corpus; must have a .words() method\n",
    "    corpus_two : NLTK corpus\n",
    "        the second corpus; must have a .words() method\n",
    "    limit : int\n",
    "        the maximum number of words to take from each corpus\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> fuse_corpora(brown, reuters, 3)\n",
    "    ['the', 'fulton', 'county', 'asian', 'exporters', 'fear']\n",
    "    \"\"\"\n",
    "    return [word.lower() for word in corpus_one.words()[:limit]] + [word.lower() for word in corpus_two.words()[:limit]]\n",
    "\n",
    "def extract_ngrams(ngram_size: int, corpus: list):\n",
    "    \"\"\"\n",
    "    Extract ngrams from a corpus.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> extract_ngrams(3, brown.words()[-5:-1])\n",
    "    [('boucle', 'dress', 'was'), ('dress', 'was', 'stupefying')]\n",
    "    \"\"\"\n",
    "    corpus = [word.lower() for word in corpus]\n",
    "\n",
    "    return [tuple(corpus[pos:pos+ngram_size]) for pos in range(len(corpus) - (ngram_size - 1))]\n",
    "\n",
    "def build_counts_model(corpus, min_size=1, max_size=MAX_NGRAM, smooth=SMOOTH):\n",
    "    \"\"\"\n",
    "    Returns the number of occurences of each ngram in a corpus, sorted by the ngram size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: list\n",
    "      the corpus from which ngrams will be extracted\n",
    "    minsize : int\n",
    "      the smallest ngram size to analyze\n",
    "    maxsize : int\n",
    "      the largest ngram size to analyze\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> build_counts_model(brown.words()[:50])\n",
    "    {1: defaultdict(<function ...>, {('the',): 3, ('fulton',): 1, ...}),\n",
    "     ...\n",
    "     4: defaultdict(<function ...>, {('the', 'fulton', 'county', 'grand'): 1, ('fulton', 'county', 'grand', 'jury'): 1, ...})}\n",
    "    \n",
    "    >>> build_counts_model(brown.words()[:50])[2][('the', 'fulton')]\n",
    "    1\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "\n",
    "    for ngram_size in range(min_size, max_size + 1):\n",
    "        raw = Counter(extract_ngrams(ngram_size, corpus)) # iterable of (ngram, count)\n",
    "        d = defaultdict(lambda: smooth)\n",
    "        for ngram, c in raw.items():\n",
    "            d[ngram] = c\n",
    "        counts[ngram_size] = d\n",
    "\n",
    "    return counts\n",
    "\n",
    "def precompute_prefix_denominators(counts: dict, max_size: int=MAX_NGRAM):\n",
    "    \"\"\"\n",
    "    Build a lookup: prefix_sums[n][prefix] = total count of n-grams starting with that prefix.\n",
    "    This makes P(word | prefix) lookup O(1).\n",
    "    \"\"\"\n",
    "    prefix_sums = {n: defaultdict(int) for n in range(2, max_size + 1)}\n",
    "\n",
    "    for n in range(2, max_size + 1):\n",
    "        counter = counts.get(n, {})\n",
    "        for ngram, freq in counter.items():\n",
    "            prefix = ngram[:-1]\n",
    "            prefix_sums[n][prefix] += freq\n",
    "\n",
    "    return prefix_sums\n",
    "\n",
    "def precompute_totals(counts: dict, smooth: float=SMOOTH):\n",
    "    \"\"\"\n",
    "    Precompute total smoothed counts for each n-gram order to speed up probability lookups.\n",
    "    Returns a dict like {1: total_unigram_mass, 2: total_bigram_mass, ...}.\n",
    "    \"\"\"\n",
    "    total_counts = {}\n",
    "    for n, counter in counts.items():\n",
    "        total_counts[n] = sum(counter.values()) + smooth * len(counter)\n",
    "    return total_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42115b",
   "metadata": {},
   "source": [
    "This third cell computes all the variables that will be used in the final model. This code takes around 30 seconds to run at minimum, so print statements are included to indicate if the cell is running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a10d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpuses built\n",
      "counts models built\n",
      "prefix denominators precomputed\n",
      "total counts precomputed\n",
      "[['K', 'AE1', 'T']]\n",
      "['cat', 'catt', 'kat', 'katt']\n"
     ]
    }
   ],
   "source": [
    "# full corpus & corpus with punctuation and stopwords removed\n",
    "corpus_full = fuse_corpora(brown, reuters)\n",
    "\n",
    "punctuation = set(string.punctuation) # also used later to determine sentence boundaries\n",
    "to_be_filtered = punctuation | set(stopwords.words(\"english\")) # tokens to be removed from full corpus\n",
    "corpus_filtered = [word for word in corpus_full if word not in to_be_filtered]\n",
    "print(\"corpuses built\")\n",
    "\n",
    "# full & filtered counts of unigrams to 4-grams\n",
    "counts_full = build_counts_model(corpus_full)\n",
    "counts_filtered = build_counts_model(corpus_filtered)\n",
    "print(\"counts models built\")\n",
    "\n",
    "# precompute prefix denominators for full & filtered counts\n",
    "prefix_denoms_full = precompute_prefix_denominators(counts_full)\n",
    "prefix_denoms_filtered = precompute_prefix_denominators(counts_filtered)\n",
    "print(\"prefix denominators precomputed\")\n",
    "\n",
    "# precompute total counts for full & filtered counts\n",
    "total_counts_full = precompute_totals(counts_full)\n",
    "total_counts_filtered = precompute_totals(counts_filtered)\n",
    "print(\"total counts precomputed\")\n",
    "\n",
    "# create pronunciation dictionary using cmudict\n",
    "pronounce_dictionary = cmudict.dict()\n",
    "print(pronounce_dictionary['cat']) # outputs [['K', 'AE1', 'T']]\n",
    "\n",
    "# create list of valid keys in pronounceDict\n",
    "validWords = list(pronounce_dictionary.keys())\n",
    "\n",
    "# create dictionary that maps pronunciations tuples to words\n",
    "reverse_pronounce_dictionary = defaultdict(list)\n",
    "\n",
    "for word, pronunciations in cmudict.dict().items():\n",
    "    for pronunciation in pronunciations:\n",
    "        reverse_pronounce_dictionary[tuple(pronunciation)].append(word)\n",
    "print(reverse_pronounce_dictionary[('K', 'AE1', 'T')]) # outputs ['cat', 'catt', 'kat', 'katt']\n",
    "\n",
    "# Collect words tagged as adjectives, prepositions, and nouns in Brown -- don't need full corpus for this, as this for ironing out common their/they're/there type errors\n",
    "adjectives = set()\n",
    "prepositions = set()\n",
    "nouns = set()\n",
    "\n",
    "for (word, tag) in brown.tagged_words():\n",
    "    if tag.startswith(\"JJ\"):\n",
    "        adjectives.add(word.lower())\n",
    "    if tag.startswith(\"IN\"):\n",
    "        prepositions.add(word.lower())\n",
    "    if tag.startswith(\"NN\"):\n",
    "        nouns.add(word.lower())\n",
    "nouns.remove(\"a\") # fixes a tagging error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36b6d4",
   "metadata": {},
   "source": [
    "The following cells contain the functions are used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b4c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0383781100283299\n",
      "0.027717523915271918\n"
     ]
    }
   ],
   "source": [
    "def prob_from_counts(word: str, context_size: int, context_data: tuple, \n",
    "                     counts: dict, prefix_sums: int, total_counts: int):\n",
    "    \"\"\"\n",
    "    Return P(word | context) estimated from counts and precomputed denominators.\n",
    "    \"\"\"\n",
    "    n = context_size + 1\n",
    "    counter = counts.get(n, Counter())\n",
    "    total = total_counts.get(n, sum(counter.values()) + SMOOTH * len(counter))\n",
    "\n",
    "    if context_size == 0:\n",
    "        # unigram probability\n",
    "        return (counter.get((word,), 0) + SMOOTH) / total\n",
    "\n",
    "    prefix = tuple(context_data[-context_size:]) if context_size <= len(context_data) else None\n",
    "    if prefix is None:\n",
    "        return SMOOTH / (total + SMOOTH) ** 2\n",
    "\n",
    "    denom = prefix_sums[n].get(prefix, 0) + SMOOTH * len(counter)\n",
    "    joint_count = counter.get(prefix + (word,), 0) + SMOOTH\n",
    "\n",
    "    if denom == 0:\n",
    "        return SMOOTH / (total + SMOOTH) ** 2\n",
    "\n",
    "    return joint_count / denom\n",
    "\n",
    "\n",
    "# Tests\n",
    "print(prob_from_counts(\"year\", 2, (\"in\", \"the\", \"last\"), counts_full, prefix_denoms_full, total_counts_full))\n",
    "print(prob_from_counts(\"day\", 2, (\"in\", \"the\", \"last\"), counts_full, prefix_denoms_full, total_counts_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'diddler']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:55: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:55: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\uddin\\AppData\\Local\\Temp\\ipykernel_8520\\402584362.py:55: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  test2 = re.findall(r\"[\\w']+|[.,!?;]\", test2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TODO: \\n-make a bunch of test cases -- seems to struggle with short sentences, especially with full counts/prefixes\\n-figure out how many words to use from unigrams list and each iteration of top_continuations\\n-figure out how to incorporate both full and filtered counts into 1 model\\n-POS tagging for sentences that result in the use of unigram predictions (unseen words in context)\\n-figure out whether or not to parse sentences for punctuation -- break for loop if ctx[-(ngram_size - 1)] in [punctuation]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nbest_continuations(sent: list, counts: dict, prefix_sums: int, total_counts: int, nbest: int=5):\n",
    "    \"\"\"\n",
    "    Outputs the most likely next word of the input sentence.\n",
    "    \"\"\"\n",
    "    ctx = [w.lower() for w in sent]\n",
    "    candidate_probs = defaultdict(float)\n",
    "\n",
    "    # Gather candidates from higher-order contexts first\n",
    "    candidates = set()\n",
    "\n",
    "    for ngram_size in range(2, min(len(ctx) + 1, MAX_NGRAM + 1)):\n",
    "        prefix = tuple(ctx[-(ngram_size - 1):])\n",
    "        \n",
    "        # collect all words that appear after this prefix\n",
    "        continuations = [\n",
    "            ngram[-1]\n",
    "            for ngram, count in counts[ngram_size].items()\n",
    "            if ngram[:-1] == prefix\n",
    "        ]\n",
    "        \n",
    "        top_continuations = sorted(\n",
    "            continuations,\n",
    "            key=lambda w: counts[ngram_size][prefix + (w,)],\n",
    "            reverse=True\n",
    "        )[:100]\n",
    "        candidates.update(top_continuations)\n",
    "    \n",
    "    # fallback to top unigrams only if no higher-order candidates were found\n",
    "    if len(candidates) == 0:\n",
    "        top_unigrams = sorted(counts[1], key=counts[1].get, reverse=True)[:300]\n",
    "        candidates.update(top_unigrams)\n",
    "\n",
    "    # Compute interpolated log probabilities\n",
    "    for w in candidates:\n",
    "        p_mix = 0.0\n",
    "        for context_size in range(MAX_NGRAM):\n",
    "            p_mix += LAMBDA[context_size] * prob_from_counts(\n",
    "                w, context_size, ctx, counts, prefix_sums, total_counts\n",
    "            )\n",
    "        candidate_probs[w] = math.log(p_mix)\n",
    "\n",
    "    ranked = sorted(candidate_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f780bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_words(word, pronounce_dict=pronounce_dictionary, reverse_pronounce_dict=reverse_pronounce_dictionary):\n",
    "    \"\"\"\n",
    "    Return all words with the same pronunciations.\n",
    "    \"\"\"\n",
    "    if word not in pronounce_dict:\n",
    "        return []\n",
    "\n",
    "    results = set()\n",
    "    for pronunciation in pronounce_dict[word]:\n",
    "        pronunciation = tuple(pronunciation)\n",
    "        \n",
    "        if pronunciation in reverse_pronounce_dict:\n",
    "            results.update(reverse_pronounce_dict[pronunciation])\n",
    "\n",
    "    return list(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290e6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextLastWordIndex(sent, current_index, punct=punctuation):\n",
    "    \"\"\"\n",
    "    Finds the index of the word before the next punctuation mark in a sentence.\n",
    "    \"\"\"\n",
    "    for i in range(current_index + 1, len(sent)):\n",
    "        if sent[i] in punct:\n",
    "            return i - 1\n",
    "        \n",
    "    return len(sent) - 1  # return last index of sentence if no punctuation found\n",
    "\n",
    "def best_word(sent: list, wordIndex, candidates,\n",
    "             counts_full, prefix_denoms_full, total_counts_full,\n",
    "             smooth=SMOOTH, lambda_vals=LAMBDA, maxSize=MAX_NGRAM, adjs=adjectives, ins=prepositions, nns=nouns):\n",
    "    \"\"\"\n",
    "    Chooses the best candidate replacement for sent[wordIndex] using n-gram context.\n",
    "    Uses both past and future context. Returns the candidate with highest log-prob.\n",
    "    \"\"\"\n",
    "\n",
    "    wordProbs = defaultdict(float)\n",
    "    stopIndex = nextLastWordIndex(sent, wordIndex)  # punctuation boundary\n",
    "\n",
    "    for cand in candidates:\n",
    "        total_logp = 0.0\n",
    "\n",
    "        # ----- 1. PAST CONTEXT -----\n",
    "        # compute P(cand | previous words)\n",
    "        for n in range(len(lambda_vals)):\n",
    "            total_logp += lambda_vals[n-1] * prob_from_counts(\n",
    "                cand,\n",
    "                n,\n",
    "                sent[:wordIndex] + [cand],\n",
    "                counts_full, prefix_denoms_full, total_counts_full\n",
    "            )\n",
    "\n",
    "        # ----- 2. FUTURE CONTEXT -----\n",
    "        # compute P(next_word | cand), P(next2 | cand next), ...\n",
    "        future_slice = sent[wordIndex+1: stopIndex]\n",
    "        context_window = [cand] + future_slice  # synthetic window\n",
    "\n",
    "        for i in range(len(context_window) - 1):\n",
    "            next_token = context_window[i + 1]\n",
    "\n",
    "            # n-gram size grows as we move right\n",
    "            n = min(maxSize, i + 2)\n",
    "\n",
    "            total_logp += prob_from_counts(\n",
    "                next_token,\n",
    "                n,\n",
    "                context_window[:i+1],\n",
    "                counts_full, prefix_denoms_full, total_counts_full,\n",
    "            )\n",
    "\n",
    "            if n == maxSize:\n",
    "                break  # no larger n-grams\n",
    "\n",
    "        wordProbs[cand] = total_logp\n",
    "\n",
    "    wordProbs[sent[wordIndex]] += smooth   # small boost for original word\n",
    "\n",
    "    # cases for THEY'RE / THEIR / THERE\n",
    "    if sent[wordIndex] in {\"they're\", 'their', 'there'} and wordIndex < len(sent) - 1:\n",
    "        for i in range(min(len(sent) - wordIndex, maxSize)):\n",
    "            if sent[wordIndex + i] in ins:\n",
    "                wordProbs[\"they're\"] += 1\n",
    "                break\n",
    "\n",
    "            if sent[wordIndex + i] in adjs:\n",
    "                continue\n",
    "\n",
    "    # pick the highest-scoring candidate\n",
    "    return max(wordProbs, key=wordProbs.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff0473",
   "metadata": {},
   "source": [
    "This next cell contains the code for the sentence prediction/spellchecking model. The following cell can be used to test it by inputting a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab71efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prediction(sent: list, punct=punctuation, nns=nouns):\n",
    "    output = []\n",
    "\n",
    "    # Localize globals for speed\n",
    "    close_words_fn = close_words\n",
    "    best_word_fn = best_word\n",
    "    nbest_fn = nbest_continuations\n",
    "    model = (counts_full, prefix_denoms_full, total_counts_full)\n",
    "    append = output.append\n",
    "    stop = nns | punct\n",
    "\n",
    "    # spellcheck first\n",
    "    for i, word in enumerate(sent):\n",
    "        # special cases: , . ? ! ; - --, etc.  (signals to end parsing)\n",
    "        if word in punct:\n",
    "            append(word)\n",
    "            continue\n",
    "            # TODO: make cases for (), which treats what's inside as its own sentence to be analzyed, but still remembers what's outside the ()\n",
    "\n",
    "        closest_words = close_words_fn(word) # TODO: prechache close words for all words in Brown corpus beforehand?\n",
    "        correct_word = best_word_fn(sent, i, closest_words, *model)\n",
    "        append(correct_word)\n",
    "\n",
    "    # predict next word until next word is noun or punctuation\n",
    "    while True:\n",
    "        print(output)\n",
    "        nbest = nbest_fn(output, *model, 1)\n",
    "        next_word = nbest[0][0]\n",
    "        append(next_word)\n",
    "        \n",
    "        if next_word in stop:\n",
    "            break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a21dd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input a sentence:\n",
      "['i', 'like', 'to']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m pattern = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+[:.]\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+(?:\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+)?|[.,!?;:()\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m-—]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m sent = re.findall(pattern, sent)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43msentence_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# predict next word\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# nbest_continuations(sent, ...) #TODO: loop function until the next word isn't in to_be_filtered / is punctiation?\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# TODO: combine functions so there aren't many nested functions\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36msentence_prediction\u001b[39m\u001b[34m(sent, punct, nns)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     nbest = \u001b[43mnbest_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     next_word = nbest[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     29\u001b[39m     append(next_word)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mnbest_continuations\u001b[39m\u001b[34m(sent, counts, prefix_sums, total_counts, nbest)\u001b[39m\n\u001b[32m     35\u001b[39m     p_mix = \u001b[32m0.0\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m context_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_NGRAM):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         p_mix += LAMBDA[context_size] * \u001b[43mprob_from_counts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_sums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_counts\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     candidate_probs[w] = math.log(p_mix)\n\u001b[32m     42\u001b[39m ranked = \u001b[38;5;28msorted\u001b[39m(candidate_probs.items(), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mprob_from_counts\u001b[39m\u001b[34m(word, context_size, context_data, counts, prefix_sums, total_counts)\u001b[39m\n\u001b[32m      6\u001b[39m n = context_size + \u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m counter = counts.get(n, Counter())\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m total = \u001b[43mtotal_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mSMOOTH\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context_size == \u001b[32m0\u001b[39m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# unigram probability\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (counter.get((word,), \u001b[32m0\u001b[39m) + SMOOTH) / total\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"input a sentence:\")\n",
    "sent = input()\n",
    "\n",
    "pattern = r\"\\d+[:.]\\d+|\\w+(?:'\\w+)?|[.,!?;:()\\-—]\"\n",
    "sent = re.findall(pattern, sent)\n",
    "\n",
    "sentence_prediction(sent)\n",
    "\n",
    "# predict next word\n",
    "# nbest_continuations(sent, ...) #TODO: loop function until the next word isn't in to_be_filtered / is punctiation?\n",
    "\n",
    "# TODO: combine functions so there aren't many nested functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
