{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43796390",
   "metadata": {},
   "source": [
    "This project is a work in progress. I want to add a feature that can predict how a misspelled word would be pronounced using the phonetic rules of English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358caac",
   "metadata": {},
   "source": [
    "This first cell contains the necessary libraries for this Jupyter notebook, as well as configuration variables to fine-tune the functions in the sentence prediction/spellchecking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc069ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\uddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict, brown, reuters, stopwords\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nltk.download(\"cmudict\")\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger', force=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', force=True)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ---------- configuration ----------\n",
    "LAMBDA = [0.05, 0.15, 0.3, 0.5] # format: [unigram_weight, bigram_weight, trigram_weight, 4gram_weight]; sum(LAMBDA) ≈ 1.0; longer n-grams have more weight\n",
    "MAX_NGRAM = len(LAMBDA)\n",
    "SMOOTH = 1e-8 # generally a fallback value in case an n-gram has a count of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b339634",
   "metadata": {},
   "source": [
    "This second cell contains all of the functions that precompute variables that are used in the model -- such as the full corpus, n-gram counts, sum of the number of each n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d765e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_corpora(corpus_one, corpus_two, limit=None):\n",
    "    \"\"\"\n",
    "    Combine two corpora (that have .words() method) into a single list of lower-cased words.\n",
    "    \"\"\"\n",
    "    return [word.lower() for word in corpus_one.words()[:limit]] + [word.lower() for word in corpus_two.words()[:limit]]\n",
    "\n",
    "def smooth_default():\n",
    "    return SMOOTH\n",
    "\n",
    "def build_counts_model(corpus, min_size=1, max_size=MAX_NGRAM, smooth=SMOOTH):\n",
    "    \"\"\"\n",
    "    Returns the number of occurences of each ngram in a corpus, sorted by the ngram size.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    L = len(corpus)\n",
    "\n",
    "    for n in range(min_size, max_size + 1):\n",
    "        counter = Counter(\n",
    "            tuple(corpus[i:i+n]) for i in range(L - n + 1)\n",
    "        )\n",
    "\n",
    "        d = defaultdict(smooth_default)\n",
    "        d.update(counter)\n",
    "        counts[n] = d\n",
    "\n",
    "    return counts\n",
    "\n",
    "def precompute_prefix_denominators(counts: dict, max_size: int = MAX_NGRAM):\n",
    "    \"\"\"\n",
    "    Build a lookup: prefix_sums[n][prefix] = total count of n-grams starting with that prefix.\n",
    "    \"\"\"\n",
    "    prefix_sums = {n: defaultdict(int) for n in range(2, max_size + 1)}\n",
    "\n",
    "    for n in range(2, max_size + 1):\n",
    "        counter = counts.get(n)\n",
    "        if not counter:\n",
    "            continue\n",
    "\n",
    "        ps = prefix_sums[n]\n",
    "        for ngram, freq in counter.items():\n",
    "            ps[ngram[:-1]] += freq\n",
    "\n",
    "    return prefix_sums\n",
    "\n",
    "def precompute_totals(counts: dict, smooth: float = SMOOTH):\n",
    "    \"\"\"\n",
    "    Precompute total smoothed counts for each n-gram order.\n",
    "    \"\"\"\n",
    "    total_counts = {}\n",
    "\n",
    "    for n, counter in counts.items():\n",
    "        counter_len = len(counter)\n",
    "        counter_sum = sum(counter.values())\n",
    "        total_counts[n] = counter_sum + smooth * counter_len\n",
    "\n",
    "    return total_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42115b",
   "metadata": {},
   "source": [
    "This third cell computes all the variables that will be used in the final model. This code takes around 30 seconds to run at minimum, so print statements are included to indicate if the cell is running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a10d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full corpus built\n",
      "filtered corpus built\n",
      "full counts models built\n",
      "filtered counts models built\n",
      "full prefix denominators precomputed\n",
      "filtered prefix denominators precomputed\n",
      "total counts precomputed\n",
      "[['K', 'AE1', 'T']]\n",
      "['cat', 'catt', 'kat', 'katt']\n"
     ]
    }
   ],
   "source": [
    "# full corpus & corpus with punctuation and stopwords removed\n",
    "corpus_full = fuse_corpora(brown, reuters)\n",
    "print(\"full corpus built\")\n",
    "\n",
    "punctuation = set(string.punctuation) # also used later to determine sentence boundaries\n",
    "to_be_filtered = punctuation | set(stopwords.words(\"english\")) # tokens to be removed from full corpus\n",
    "corpus_filtered = [word for word in corpus_full if word not in to_be_filtered]\n",
    "print(\"filtered corpus built\")\n",
    "\n",
    "# full & filtered counts of unigrams to 4-grams\n",
    "counts_full = build_counts_model(corpus_full)\n",
    "print(\"full counts models built\")\n",
    "counts_filtered = build_counts_model(corpus_filtered)\n",
    "print(\"filtered counts models built\")\n",
    "\n",
    "# precompute prefix denominators for full & filtered counts\n",
    "prefix_denoms_full = precompute_prefix_denominators(counts_full)\n",
    "print(\"full prefix denominators precomputed\")\n",
    "prefix_denoms_filtered = precompute_prefix_denominators(counts_filtered)\n",
    "print(\"filtered prefix denominators precomputed\")\n",
    "\n",
    "# precompute total counts for full & filtered counts\n",
    "total_counts_full = precompute_totals(counts_full)\n",
    "total_counts_filtered = precompute_totals(counts_filtered)\n",
    "print(\"total counts precomputed\")\n",
    "\n",
    "# create pronunciation dictionary using cmudict\n",
    "pronounce_dictionary = cmudict.dict()\n",
    "print(pronounce_dictionary['cat']) # outputs [['K', 'AE1', 'T']]\n",
    "\n",
    "# create list of valid keys in pronounceDict\n",
    "valid_words = list(pronounce_dictionary.keys())\n",
    "\n",
    "# create dictionary that maps pronunciations tuples to words\n",
    "reverse_pronounce_dictionary = defaultdict(list)\n",
    "for word, pronunciations in pronounce_dictionary.items():\n",
    "    for pronunciation in pronunciations:\n",
    "        reverse_pronounce_dictionary[tuple(pronunciation)].append(word)\n",
    "print(reverse_pronounce_dictionary[('K', 'AE1', 'T')]) # outputs ['cat', 'catt', 'kat', 'katt']\n",
    "\n",
    "# Collect words tagged as adjectives, prepositions, and nouns in Brown -- don't need full corpus for this, as this for ironing out common their/they're/there type errors\n",
    "adjectives = set()\n",
    "prepositions = set()\n",
    "nouns = set()\n",
    "\n",
    "for (word, tag) in brown.tagged_words():\n",
    "    word_lc = word.lower()\n",
    "    if tag.startswith(\"JJ\"):\n",
    "        adjectives.add(word_lc)\n",
    "    if tag.startswith(\"IN\"):\n",
    "        prepositions.add(word_lc)\n",
    "    if tag.startswith(\"NN\"):\n",
    "        nouns.add(word_lc)\n",
    "nouns.remove(\"a\") # fixes a tagging error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36b6d4",
   "metadata": {},
   "source": [
    "The following cells contain the functions are used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51b4c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03823796095285799\n",
      "0.02761630572271161\n"
     ]
    }
   ],
   "source": [
    "def prob_from_counts(word: str, context_size: int, context_data: tuple, \n",
    "                     counts: dict, prefix_sums: dict, total_counts: dict, SMOOTH=1e-6):\n",
    "    \"\"\"\n",
    "    Return P(word | context) estimated from counts and precomputed denominators.\n",
    "    \"\"\"\n",
    "    n = context_size + 1\n",
    "    counter = counts.get(n)\n",
    "    if counter is None:\n",
    "        counter = Counter()\n",
    "    total = total_counts.get(n)\n",
    "    if total is None:\n",
    "        total = sum(counter.values()) + SMOOTH * len(counter)\n",
    "    \n",
    "    if context_size == 0:\n",
    "        # unigram probability\n",
    "        return (counter.get((word,), 0) + SMOOTH) / total\n",
    "\n",
    "    # Compute prefix\n",
    "    if context_size <= len(context_data):\n",
    "        prefix = tuple(context_data[-context_size:])\n",
    "    else:\n",
    "        return SMOOTH / (total + SMOOTH)\n",
    "\n",
    "    # Get denominator for the conditional probability\n",
    "    denom = prefix_sums.get(n, {}).get(prefix, 0) + SMOOTH * len(counter)\n",
    "    joint_count = counter.get(prefix + (word,), 0) + SMOOTH\n",
    "\n",
    "    # Safe fallback\n",
    "    if denom == 0:\n",
    "        return SMOOTH / (total + SMOOTH)\n",
    "\n",
    "    return joint_count / denom\n",
    "\n",
    "\n",
    "\n",
    "# Tests\n",
    "print(prob_from_counts(\"year\", 2, (\"in\", \"the\", \"last\"), counts_full, prefix_denoms_full, total_counts_full))\n",
    "print(prob_from_counts(\"day\", 2, (\"in\", \"the\", \"last\"), counts_full, prefix_denoms_full, total_counts_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a6000a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8671374524822032e-13\n",
      "1.8671393196196555e-07\n"
     ]
    }
   ],
   "source": [
    "print(prob_from_counts(\"time\", 3, (\"they're\", \"good\", \"at\", \"that\"), counts_full, prefix_denoms_full, total_counts_full))\n",
    "print(prob_from_counts(\".\", 3, (\"they're\", \"good\", \"at\", \"that\"), counts_full, prefix_denoms_full, total_counts_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8312c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbest_continuations(sent: list, counts: dict, prefix_sums: int, total_counts: int, nbest: int=5):\n",
    "    \"\"\"\n",
    "    Outputs the most likely next word of the input sentence.\n",
    "    \"\"\"\n",
    "    ctx = [w.lower() for w in sent]\n",
    "    candidate_probs = defaultdict(float)\n",
    "\n",
    "    # Gather candidates from higher-order contexts first\n",
    "    candidates = set()\n",
    "\n",
    "    for ngram_size in range(2, min(len(ctx) + 1, MAX_NGRAM + 1)):\n",
    "        prefix = tuple(ctx[-(ngram_size - 1):])\n",
    "        \n",
    "        # collect all words that appear after this prefix\n",
    "        continuations = [\n",
    "            ngram[-1]\n",
    "            for ngram, count in counts[ngram_size].items()\n",
    "            if ngram[:-1] == prefix\n",
    "        ]\n",
    "        \n",
    "        top_continuations = sorted(\n",
    "            continuations,\n",
    "            key=lambda w: counts[ngram_size][prefix + (w,)],\n",
    "            reverse=True\n",
    "        )[:100]\n",
    "        candidates.update(top_continuations)\n",
    "    \n",
    "    # fallback to top unigrams only if no higher-order candidates were found\n",
    "    if len(candidates) == 0:\n",
    "        top_unigrams = sorted(counts[1], key=counts[1].get, reverse=True)[:300]\n",
    "        candidates.update(top_unigrams)\n",
    "\n",
    "    # Compute interpolated log probabilities\n",
    "    for w in candidates:\n",
    "        p_mix = 0.0\n",
    "        for context_size in range(MAX_NGRAM):\n",
    "            p_mix += LAMBDA[context_size] * prob_from_counts(\n",
    "                w, context_size, ctx, counts, prefix_sums, total_counts\n",
    "            )\n",
    "        candidate_probs[w] = math.log(p_mix)\n",
    "\n",
    "    ranked = sorted(candidate_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f780bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_words(word, pronounce_dict=pronounce_dictionary, reverse_pronounce_dict=reverse_pronounce_dictionary):\n",
    "    \"\"\"\n",
    "    Return all words with the same pronunciations.\n",
    "    \"\"\"\n",
    "    if word not in pronounce_dict:\n",
    "        return []\n",
    "\n",
    "    results = set()\n",
    "    for pronunciation in pronounce_dict[word]:\n",
    "        pronunciation = tuple(pronunciation)\n",
    "        \n",
    "        if pronunciation in reverse_pronounce_dict:\n",
    "            results.update(reverse_pronounce_dict[pronunciation])\n",
    "\n",
    "    return list(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290e6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextLastWordIndex(sent, current_index, punct=punctuation):\n",
    "    \"\"\"\n",
    "    Finds the index of the word before the next punctuation mark in a sentence.\n",
    "    \"\"\"\n",
    "    for i in range(current_index + 1, len(sent)):\n",
    "        if sent[i] in punct:\n",
    "            return i - 1\n",
    "        \n",
    "    return len(sent) - 1  # return last index of sentence if no punctuation found\n",
    "\n",
    "def best_word(sent: list, wordIndex, candidates,\n",
    "             counts_full, prefix_denoms_full, total_counts_full,\n",
    "             smooth=SMOOTH, lambda_vals=LAMBDA, maxSize=MAX_NGRAM, adjs=adjectives, ins=prepositions, nns=nouns):\n",
    "    \"\"\"\n",
    "    Chooses the best candidate replacement for sent[wordIndex] using n-gram context.\n",
    "    Uses both past and future context. Returns the candidate with highest log-prob.\n",
    "    \"\"\"\n",
    "\n",
    "    wordProbs = defaultdict(float)\n",
    "    stopIndex = nextLastWordIndex(sent, wordIndex)  # punctuation boundary\n",
    "\n",
    "    for cand in candidates:\n",
    "        total_logp = 0.0\n",
    "\n",
    "        # ----- 1. PAST CONTEXT -----\n",
    "        # compute P(cand | previous words)\n",
    "        for n in range(len(lambda_vals)):\n",
    "            total_logp += lambda_vals[n-1] * prob_from_counts(\n",
    "                cand,\n",
    "                n,\n",
    "                sent[:wordIndex] + [cand],\n",
    "                counts_full, prefix_denoms_full, total_counts_full\n",
    "            )\n",
    "\n",
    "        # ----- 2. FUTURE CONTEXT -----\n",
    "        # compute P(next_word | cand), P(next2 | cand next), ...\n",
    "        future_slice = sent[wordIndex+1: stopIndex]\n",
    "        context_window = [cand] + future_slice  # synthetic window\n",
    "\n",
    "        for i in range(len(context_window) - 1):\n",
    "            next_token = context_window[i + 1]\n",
    "\n",
    "            # n-gram size grows as we move right\n",
    "            n = min(maxSize, i + 2)\n",
    "\n",
    "            total_logp += prob_from_counts(\n",
    "                next_token,\n",
    "                n,\n",
    "                context_window[:i+1],\n",
    "                counts_full, prefix_denoms_full, total_counts_full,\n",
    "            )\n",
    "\n",
    "            if n == maxSize:\n",
    "                break  # no larger n-grams\n",
    "\n",
    "        wordProbs[cand] = total_logp\n",
    "\n",
    "    wordProbs[sent[wordIndex]] += smooth   # small boost for original word\n",
    "\n",
    "    # cases for THEY'RE / THEIR / THERE\n",
    "    if sent[wordIndex] in {\"they're\", 'their', 'there'} and wordIndex < len(sent) - 1:\n",
    "        for i in range(min(len(sent) - wordIndex, maxSize)):\n",
    "            if sent[wordIndex + i] in ins:\n",
    "                wordProbs[\"they're\"] += 1\n",
    "                break\n",
    "\n",
    "            if sent[wordIndex + i] in adjs:\n",
    "                continue\n",
    "\n",
    "    # pick the highest-scoring candidate\n",
    "    return max(wordProbs, key=wordProbs.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff0473",
   "metadata": {},
   "source": [
    "This next cell contains the code for the sentence prediction/spellchecking model. The following cell can be used to test it by inputting a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ab71efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prediction(sent: list, punct=punctuation, nns=nouns, prediction_length=5):\n",
    "    output = []\n",
    "\n",
    "    # Localize globals for speed\n",
    "    close_words_fn = close_words\n",
    "    best_word_fn = best_word\n",
    "    nbest_fn = nbest_continuations\n",
    "    model = (counts_full, prefix_denoms_full, total_counts_full)\n",
    "    append = output.append\n",
    "    stop = nns | punct\n",
    "\n",
    "    # spellcheck first\n",
    "    for i, word in enumerate(sent):\n",
    "        # special cases: , . ? ! ; - --, etc.  (signals to end parsing)\n",
    "        if word in punct:\n",
    "            append(word)\n",
    "            continue\n",
    "            # TODO: make cases for (), which treats what's inside as its own sentence to be analzyed, but still remembers what's outside the ()\n",
    "\n",
    "        closest_words = close_words_fn(word) # TODO: prechache close words for all words in Brown corpus beforehand?\n",
    "        correct_word = best_word_fn(sent, i, closest_words, *model)\n",
    "        append(correct_word)\n",
    "\n",
    "    # predict next word until next word is noun or punctuation\n",
    "    for _ in range(prediction_length):\n",
    "        print(output)\n",
    "        nbest = nbest_fn(output, *model, 1)\n",
    "        next_word = nbest[0][0]\n",
    "        append(next_word)\n",
    "        \n",
    "        if next_word in stop:\n",
    "            break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a21dd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input a sentence:\n",
      "['i', 'think', \"they're\", 'good', 'at']\n",
      "['i', 'think', \"they're\", 'good', 'at', 'that']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'think', \"they're\", 'good', 'at', 'that', 'time']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"input a sentence:\")\n",
    "sent = input()\n",
    "\n",
    "pattern = r\"\\d+[:.]\\d+|\\w+(?:'\\w+)?|[.,!?;:()\\-—]\"\n",
    "sent = re.findall(pattern, sent)\n",
    "\n",
    "sentence_prediction(sent)\n",
    "\n",
    "# predict next word\n",
    "# nbest_continuations(sent, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
